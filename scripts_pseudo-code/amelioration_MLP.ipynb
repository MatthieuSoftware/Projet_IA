{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Exploration de différentes architectures ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matt-\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision avec une couche cachée de 100 neurones : 96.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matt-\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision avec deux couches cachées (100, 50 neurones) : 97.24%\n",
      "\n",
      "--- Introduction à la régularisation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matt-\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision avec régularisation L2 (alpha=0.001) : 97.09%\n",
      "\n",
      "--- Exploration de différents algorithmes d'optimisation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matt-\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision avec l'optimiseur Adam : 96.84%\n",
      "Précision avec l'optimiseur lbfsg (taux d'apprentissage=0.01) : 86.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matt-\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Author : Badr TAJINI\n",
    "#\n",
    "'''\n",
    "# Code Python très simple pour construire un modèle MLP basique avec l'amélioration de la précision\n",
    "#\n",
    "# Objectifs de l'Étape 4 :\n",
    "#\n",
    "# 1- Impact de l'Architecture : Comprendre comment la complexité du réseau neuronal (nombre de couches et de neurones) influence sa capacité à apprendre et sa précision.\n",
    "# 2- Régularisation : Introduire le concept de régularisation comme moyen de prévenir le sur-apprentissage et d'améliorer la généralisation du modèle.\n",
    "# 3- Algorithmes d'Optimisation : Explorer différents algorithmes d'optimisation et comprendre que le choix de l'optimiseur et de ses paramètres (comme le taux d'apprentissage) peut avoir un impact significatif sur l'entraînement et les performances du modèle.\n",
    "# 4- Hyperparamètres et Ajustement : Renforcer l'idée que la construction d'un bon modèle implique de choisir et d'ajuster les bons hyperparamètres.\n",
    "# 5- Évaluation Comparative : Apprendre à comparer les performances de différents modèles et configurations.\n",
    "# \n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Instructions :\n",
    "#\n",
    "# 1- Exécutez le code.\n",
    "# 2- Observez les sorties :\n",
    "#    - Comment la précision change-t-elle lorsque vous modifiez l'architecture du modèle (plus de neurones, plus de couches) ?\n",
    "#    - Quel est l'impact de l'ajout de la régularisation L2 ? Comprenez-vous l'idée de pénaliser les poids importants pour éviter le sur-apprentissage ?\n",
    "#    - Comment les différents algorithmes d'optimisation (adam vs sgd) affectent-ils la précision ?\n",
    "# 3- Expérimentez :\n",
    "#    - Architecture : Essayez différentes configurations pour hidden_layer_sizes. Par exemple, (50, 50), (150,), (128, 64, 32). Y a-t-il une limite au nombre de couches ou de neurones que vous pouvez ajouter ?\n",
    "#    - Régularisation : Modifiez la valeur du paramètre alpha. Qu'arrive-t-il à la précision si vous augmentez ou diminuez alpha ?\n",
    "#    - Optimisation :\n",
    "#      - Pour l'optimiseur sgd, essayez différents taux d'apprentissage (learning_rate_init). Un taux plus élevé permet-il d'apprendre plus vite ? Est-ce toujours bénéfique ?\n",
    "#      - Recherchez d'autres optimiseurs disponibles dans ccc (par exemple, lbfgs, bien que moins adapté aux grands datasets) et testez-les.\n",
    "#    - Nombre d'itérations : Si vos expériences sont rapides, essayez d'augmenter max_iter pour voir si le modèle continue de s'améliorer. Soyez patients, l'entraînement peut prendre plus de temps.\n",
    "'''\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# 1. Charger et préparer le dataset MNIST (comme à l'étape 3)\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X = mnist.data.astype(np.float32) / 255.0\n",
    "y = mnist.target.astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Explorer différentes architectures de modèles MLP\n",
    "print(\"\\n--- Exploration de différentes architectures ---\")\n",
    "\n",
    "# Modèle 1 : Plus de neurones dans une seule couche cachée\n",
    "mlp_large = MLPClassifier(hidden_layer_sizes=(50, ), max_iter=10, random_state=42)\n",
    "mlp_large.fit(X_train, y_train)\n",
    "y_pred_large = mlp_large.predict(X_test)\n",
    "accuracy_large = accuracy_score(y_test, y_pred_large)\n",
    "print(f\"Précision avec une couche cachée de 100 neurones : {accuracy_large * 100:.2f}%\")\n",
    "\n",
    "# Modèle 2 : Plusieurs couches cachées\n",
    "mlp_multi = MLPClassifier(hidden_layer_sizes=(128, ), max_iter=10, random_state=42)\n",
    "mlp_multi.fit(X_train, y_train)\n",
    "y_pred_multi = mlp_multi.predict(X_test)\n",
    "accuracy_multi = accuracy_score(y_test, y_pred_multi)\n",
    "print(f\"Précision avec deux couches cachées (100, 50 neurones) : {accuracy_multi * 100:.2f}%\")\n",
    "\n",
    "# 3. Introduction à la régularisation (L2) pour éviter le sur-apprentissage\n",
    "print(\"\\n--- Introduction à la régularisation ---\")\n",
    "\n",
    "# Modèle 3 : Avec régularisation L2 (paramètre alpha)\n",
    "mlp_regularized = MLPClassifier(hidden_layer_sizes=(128,64, 32 ), max_iter=10, alpha=0.011, random_state=42)\n",
    "mlp_regularized.fit(X_train, y_train)\n",
    "y_pred_regularized = mlp_regularized.predict(X_test)\n",
    "accuracy_regularized = accuracy_score(y_test, y_pred_regularized)\n",
    "print(f\"Précision avec régularisation L2 (alpha=0.001) : {accuracy_regularized * 100:.2f}%\")\n",
    "\n",
    "# 4. Explorer différents algorithmes d'optimisation\n",
    "print(\"\\n--- Exploration de différents algorithmes d'optimisation ---\")\n",
    "\n",
    "# Modèle 4 : Utilisation de l'optimiseur 'adam' (qui est l'optimiseur par défaut)\n",
    "mlp_adam = MLPClassifier(hidden_layer_sizes=(50, 50), max_iter=20, solver='adam', random_state=42)\n",
    "mlp_adam.fit(X_train, y_train)\n",
    "y_pred_adam = mlp_adam.predict(X_test)\n",
    "accuracy_adam = accuracy_score(y_test, y_pred_adam)\n",
    "print(f\"Précision avec l'optimiseur Adam : {accuracy_adam * 100:.2f}%\")\n",
    "\n",
    "# Modèle 5 : Utilisation de l'optimiseur 'sgd' (Stochastic Gradient Descent) lbfgsvec un taux d'apprentissage\n",
    "mlp_sgd = MLPClassifier(hidden_layer_sizes=(50, 50 ), max_iter=20, solver='lbfgs', learning_rate_init=0.0001, random_state=42)\n",
    "mlp_sgd.fit(X_train, y_train)\n",
    "y_pred_sgd = mlp_sgd.predict(X_test)\n",
    "accuracy_sgd = accuracy_score(y_test, y_pred_sgd)\n",
    "print(f\"Précision avec l'optimiseur lbfsg (taux d'apprentissage=0.01) : {accuracy_sgd * 100:.2f}%\")\n",
    "\n",
    "# Remarque : `max_iter` est toujours limité ici pour des raisons de temps d'exécution lors des tests.\n",
    "# Pour obtenir de meilleures performances, il faudrait augmenter le nombre d'itérations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation : Ce script pyhton va tester différente configuraison d'un réseaux de neuronnes.\n",
    "\n",
    "-   Le modele 1 possede 1 couche caché qui possède elle meme 100 neuronnes. sont taux de réussite est de 97.09 %\n",
    "-   Le modele 2 possede 2 contenant deux couche un couche possède 100 neurones et l'autre 50 neuronnes.\n",
    "\n",
    "En regardant ces deux mdoele on remarque que le modèle ayant deux a une précision plus grande.Cela s'explique car il peut mieux comprendre et modéliser les données complexes \n",
    "-   Le modèle 3 utilise la méthode dite de la régulation .cette methode ser a controler l'entrainement des données afin que le modèle ne se spécialise trop dans des données et ainsi perdre sa généralité.\n",
    "\n",
    "La régulation va utiliser le parametre alpha qui va pénaliser le modèle afin d'éviter le surapprentissage.\n",
    "L'alghoritme de d'opitmisation adam à pour abjectif d'juster le taux d'apprentissage de chauque paramètre  du modèle de maniere adaptative.\n",
    "\n",
    "la SGD à pour objéctif de minimiser la fonction cout du modèle pour savoir la marge qu'il y a entre la réalité et les prédiction.\n",
    "\n",
    "Expérimentation : \n",
    "\n",
    "-   hidden_layer_size : Si on divise nos données en 50/50  il y aura un diminution du atux d'apprentissage car modele ne se saura pas assez entrainé avec des données disponible. \n",
    "-   hidden_layer_size : si on sépare les données en 150, il y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
